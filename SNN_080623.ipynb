{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHcIgskYt7Kp+yXJJKBlmG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MyatToe/100days-python/blob/main/SNN_080623.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch --quiet"
      ],
      "metadata": {
        "id": "8gAGK63FCD4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysZcUiO6Awll"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import random\n",
        "import statistics\n",
        "import tqdm\n",
        "print('Input library finished')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start = time.time()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)\n"
      ],
      "metadata": {
        "id": "byaj3xN1A1MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "nKN5isTOsXp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,csv_path_inp,csv_path_outp):\n",
        "        df_inp = pd.read_csv(csv_path_inp)\n",
        "        df_outp = pd.read_csv(csv_path_outp)\n",
        "        self.inp = df_inp.iloc[:,:].values\n",
        "        self.outp = df_outp.iloc[:,:].values\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.inp)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        inp = torch.FloatTensor(self.inp[idx])\n",
        "        outp = torch.FloatTensor(self.outp[idx])\n",
        "        # Convert the input and output to spikes using a suitable method\n",
        "        inp_spikes = self.convert_to_spikes(inp)\n",
        "        outp_spikes = self.convert_to_spikes(outp)\n",
        "        return inp_spikes, outp_spikes\n",
        "\n",
        "    def convert_to_spikes(self, data):\n",
        "        # Implement your spike conversion logic here\n",
        "        threshold = 0.5  # Example threshold for spike generation\n",
        "        spikes = torch.where(data >= threshold, torch.ones_like(data), torch.zeros_like(data))\n",
        "        return spikes\n",
        "\n",
        "### Need to convert construct Model\n",
        "class SNNNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SNNNet, self).__init__()\n",
        "        self.spike_relu = nn.ReLU().to(device)  # ReLU activation for spike generation\n",
        "        self.spike_tanh = nn.Tanh().to(device)  # Tanh activation for spike generation\n",
        "        self.spike_linear1 = nn.Linear(16, 12).to(device)\n",
        "        self.spike_linear2 = nn.Linear(12, 12).to(device)\n",
        "        self.spike_linear3 = nn.Linear(12, 12).to(device)\n",
        "        self.spike_linear4 = nn.Linear(12, 12).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spikes = x.float().to(device)  # Assuming the input is spike-encoded\n",
        "        h1 = self.spike_tanh(self.spike_linear1(spikes.view(-1, 16)))  # Spike-based linear and activation\n",
        "        h2 = self.spike_tanh(self.spike_linear2(h1))\n",
        "        h3 = self.spike_tanh(self.spike_linear3(h2))\n",
        "        h4 = self.spike_linear4(h3)\n",
        "        return h4\n",
        "print('Defining neural network is finished')"
      ],
      "metadata": {
        "id": "A7brjODAA36x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-xzP8UrqA5fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path_inp1 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/simple_manuevers_inp_deleted_150.csv'\n",
        "csv_path_outp1 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/simple_manuevers_outp_deleted_150.csv'\n",
        "estimate_1 = CustomDataset(csv_path_inp1, csv_path_outp1)\n",
        "dataloader_1 = DataLoader(estimate_1, batch_size=1)\n",
        "\n",
        "csv_path_inp2 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/20230324_LQR_KP2 (3)_inp_deleted_250.csv'\n",
        "csv_path_outp2 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/20230324_LQR_KP2 (3)_outp_deleted_250.csv'\n",
        "estimate_2 = CustomDataset(csv_path_inp2, csv_path_outp2)\n",
        "dataloader_2 = DataLoader(estimate_2, batch_size=1)\n",
        "\n",
        "csv_path_inp3 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/simple_manuevers_inp_deleted_300.csv'\n",
        "csv_path_outp3 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/simple_manuevers_outp_deleted_300.csv'\n",
        "estimate_3 = CustomDataset(csv_path_inp3, csv_path_outp3)\n",
        "dataloader_3 = DataLoader(estimate_3, batch_size=1)\n",
        "\n",
        "csv_path_inp4 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/20230324_LQR_KP2 (3)_inp_deleted_500.csv'\n",
        "csv_path_outp4 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/20230324_LQR_KP2 (3)_outp_deleted_500.csv'\n",
        "estimate_4 = CustomDataset(csv_path_inp4, csv_path_outp4)\n",
        "dataloader_4 = DataLoader(estimate_4, batch_size=1)\n",
        "\n",
        "csv_path_inp5 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/8_windy_maneuver_inp.csv'\n",
        "csv_path_outp5 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/8_windy_maneuver_outp.csv'\n",
        "estimate_5 = CustomDataset(csv_path_inp5, csv_path_outp5)\n",
        "dataloader_5 = DataLoader(estimate_5, batch_size=1)\n",
        "\n",
        "csv_path_inp9 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230517/20221103_LQR_KP2 (2)_inp_half.csv'\n",
        "csv_path_outp9 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230517/20221103_LQR_KP2 (2)_outp_half.csv'\n",
        "estimate_9 = CustomDataset(csv_path_inp9, csv_path_outp9)\n",
        "dataloader_9 = DataLoader(estimate_9, batch_size=1)\n",
        "\n",
        "csv_path_inp10 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_deleted/simple_manuevers_inp_deleted_150.csv'\n",
        "csv_path_outp10 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_deleted/simple_manuevers_outp_deleted_150.csv'\n",
        "estimate_10 = CustomDataset(csv_path_inp10, csv_path_outp10)\n",
        "dataloader_10 = DataLoader(estimate_10, batch_size=1)\n",
        "\n",
        "print('Data path showing finished')"
      ],
      "metadata": {
        "id": "ztijNHmVA63x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "model = SNNNet().to(device)\n",
        "loss_fn = nn.MSELoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def train_150(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Spike neural network forward pass\n",
        "        spikes = model(inp)\n",
        "        loss = loss_fn(spikes, outp)\n",
        "        \n",
        "        # Spike neural network backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_150_5.csv','a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss])        \n",
        "                \n",
        "def train_250(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Spike neural network forward pass\n",
        "        spikes = model(inp)\n",
        "        loss = loss_fn(spikes, outp)\n",
        "        \n",
        "        # Spike neural network backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_150_5.csv','a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss])    \n",
        "                \n",
        "def train_300(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Spike neural network forward pass\n",
        "        spikes = model(inp)\n",
        "        loss = loss_fn(spikes, outp)\n",
        "        \n",
        "        # Spike neural network backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_150_5.csv','a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss])      \n",
        "                \n",
        "def train_500(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Spike neural network forward pass\n",
        "        spikes = model(inp)\n",
        "        loss = loss_fn(spikes, outp)\n",
        "        \n",
        "        # Spike neural network backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_150_5.csv','a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss])       \n",
        "                \n",
        "                \n",
        "def train(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Spike neural network forward pass\n",
        "        spikes = model(inp)\n",
        "        loss = loss_fn(spikes, outp)\n",
        "        \n",
        "        # Spike neural network backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_150_5.csv','a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss])    \n",
        "    print('Data is loaded, Loss defined.')\n",
        "    "
      ],
      "metadata": {
        "id": "s-NHwukyA8Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_batch = iter(dataloader_2)\n",
        "# num_timesteps = 100  # Define the number of time steps\n",
        "\n",
        "# # Run a single forward-pass\n",
        "# with torch.no_grad():\n",
        "#     for feature, label in train_batch:\n",
        "#         feature = torch.swapaxes(input=feature, axis0=0, axis1=1)\n",
        "#         label = torch.swapaxes(input=label, axis0=0, axis1=1)\n",
        "#         feature = feature.to(device)\n",
        "#         label = label.to(device)\n",
        "\n",
        "#         # Initialize membrane potential and spike recording\n",
        "#         spk_recording = []\n",
        "#         mem = None\n",
        "\n",
        "#         # Simulate spiking neural network over time steps\n",
        "#         for step in range(num_timesteps):\n",
        "#             mem, spk = model(feature, mem)\n",
        "#             spk_recording.append(spk)\n",
        "\n",
        "#         # Convert spike recordings to membrane potentials\n",
        "#         mem = model.spike_to_membrane(mem)\n",
        "\n",
        "#         # Plot\n",
        "#         plt.plot(mem[:, 0, 0].cpu(), label=\"Output\")\n",
        "#         plt.plot(label[:, 0, 0].cpu(), '--', label=\"Target\")\n",
        "#         plt.title(\"Untrained Output Neuron\")\n",
        "#         plt.xlabel(\"Time\")\n",
        "#         plt.ylabel(\"Membrane Potential\")\n",
        "#         plt.legend(loc='best')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "dKhpYSZqzWI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "for t in range(epochs):    \n",
        "    \n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")    \n",
        "    # print(f\"Episode 1 {t+1} Training\")\n",
        "    # start = time.time()\n",
        "    # train(dataloader_5, model, loss_func_MSE, optimizer, t)\n",
        "    # print(\"time :\", time.time() - start)\n",
        "    # print('\\n') \n",
        "    \n",
        "    print(f\"Episode 1 {t+1} Training\")\n",
        "    start = time.time()\n",
        "    train(dataloader_2, model, loss_fn, optimizer, t)\n",
        "    print(\"time :\", time.time() - start)\n",
        "    print('\\n') \n",
        "    torch.save(model, f'./230522_KP2_DNN_{epochs}_250_7.pt')\n",
        "    \n",
        "    with open('./Training Time_230522_ND_250.csv','a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([time.time() - start])\n",
        "print(\"Done!\")\n",
        "\n",
        "## prediction code"
      ],
      "metadata": {
        "id": "Zy8_7rQ_A_m5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}