{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+E9cImalsaPtsgJWbURyD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MyatToe/100days-python/blob/main/SNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch --quiet"
      ],
      "metadata": {
        "id": "8gAGK63FCD4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysZcUiO6Awll"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import snntorch as snn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import time\n",
        "print('Input library finished')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start = time.time()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "print('device : ', device)\n"
      ],
      "metadata": {
        "id": "byaj3xN1A1MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv_path_inp, csv_path_outp):\n",
        "        df_inp = pd.read_csv(csv_path_inp)\n",
        "        df_outp = pd.read_csv(csv_path_outp)\n",
        "        self.inp = df_inp.iloc[:, :].values\n",
        "        self.outp = df_outp.iloc[:, :].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inp)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp = torch.FloatTensor(self.inp[idx])\n",
        "        outp = torch.FloatTensor(self.outp[idx])\n",
        "        return inp, outp\n",
        "\n",
        "class snn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(snn, self).__init__()\n",
        "        self.fc1 = nn.Linear(16, 12)\n",
        "        self.fc2 = nn.Linear(12, 12)\n",
        "        self.fc3 = nn.Linear(12, 12)\n",
        "        self.fc4 = nn.Linear(12, 12)\n",
        "\n",
        "def forward(self, x):\n",
        "    x = x.float()\n",
        "    h1 = F.relu(self.fc1(x.view(-1, 16))).to(device)\n",
        "    h2 = F.relu(self.fc2(h1)).to(device)\n",
        "    h3 = F.relu(self.fc3(h2)).to(device)\n",
        "    h4 = self.fc4(h3)\n",
        "    return h4.to(device)\n",
        "\n",
        "    def spike_encode(self, x):\n",
        "        return torch.gt(x, 0).float()\n",
        "\n",
        "class snn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(snn, self).__init__()\n",
        "        self.spiking_net = snn()\n",
        "\n",
        "    def forward(self, x, time_steps):\n",
        "        membrane_potential = torch.zeros_like(x)\n",
        "        spikes = []\n",
        "        for t in range(time_steps):\n",
        "            spike = self.spiking_net.spike_encode(membrane_potential)\n",
        "            spikes.append(spike)\n",
        "            membrane_potential = membrane_potential + self.spiking_net(spike)\n",
        "        return torch.stack(spikes, dim=0).sum(dim=0)\n",
        "\n",
        "print('Defining SNN is finished')"
      ],
      "metadata": {
        "id": "A7brjODAA36x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-xzP8UrqA5fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path_inp1 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/simple_manuevers_inp_deleted_150.csv'\n",
        "csv_path_outp1 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/simple_manuevers_outp_deleted_150.csv'\n",
        "estimate_1 = CustomDataset(csv_path_inp1, csv_path_outp1)\n",
        "dataloader_1 = DataLoader(estimate_1, batch_size=1)\n",
        "\n",
        "csv_path_inp2 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/20230324_LQR_KP2 (3)_inp_deleted_250.csv'\n",
        "csv_path_outp2 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/20230324_LQR_KP2 (3)_outp_deleted_250.csv'\n",
        "estimate_2 = CustomDataset(csv_path_inp2, csv_path_outp2)\n",
        "dataloader_2 = DataLoader(estimate_2, batch_size=1)\n",
        "\n",
        "csv_path_inp3 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/simple_manuevers_inp_deleted_300.csv'\n",
        "csv_path_outp3 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/simple_manuevers_outp_deleted_300.csv'\n",
        "estimate_3 = CustomDataset(csv_path_inp3, csv_path_outp3)\n",
        "dataloader_3 = DataLoader(estimate_3, batch_size=1)\n",
        "\n",
        "csv_path_inp4 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/20230324_LQR_KP2 (3)_inp_deleted_500.csv'\n",
        "csv_path_outp4 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/20230324_LQR_KP2 (3)_outp_deleted_500.csv'\n",
        "estimate_4 = CustomDataset(csv_path_inp4, csv_path_outp4)\n",
        "dataloader_4 = DataLoader(estimate_4, batch_size=1)\n",
        "\n",
        "csv_path_inp5 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/8_windy_maneuver_inp.csv'\n",
        "csv_path_outp5 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230522/8_windy_maneuver_outp.csv'\n",
        "estimate_5 = CustomDataset(csv_path_inp5, csv_path_outp5)\n",
        "dataloader_5 = DataLoader(estimate_5, batch_size=1)\n",
        "\n",
        "csv_path_inp9 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230517/20221103_LQR_KP2 (2)_inp_half.csv'\n",
        "csv_path_outp9 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_230517/20221103_LQR_KP2 (2)_outp_half.csv'\n",
        "estimate_9 = CustomDataset(csv_path_inp9, csv_path_outp9)\n",
        "dataloader_9 = DataLoader(estimate_9, batch_size=1)\n",
        "\n",
        "csv_path_inp10 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_deleted/simple_manuevers_inp_deleted_150.csv'\n",
        "csv_path_outp10 = '/content/drive/MyDrive/Dataset_KP2/Dataset_KP2/10Hz_deleted/simple_manuevers_outp_deleted_150.csv'\n",
        "estimate_10 = CustomDataset(csv_path_inp10, csv_path_outp10)\n",
        "dataloader_10 = DataLoader(estimate_10, batch_size=1)\n",
        "\n",
        "print('Data path showing finished')"
      ],
      "metadata": {
        "id": "ztijNHmVA63x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class snn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(snn, self).__init__()\n",
        "        self.spiking_net = snn()\n",
        "\n",
        "    def forward(self, x):\n",
        "        membrane_potential = torch.zeros_like(x)\n",
        "        spikes = []\n",
        "        for t in range(10):\n",
        "            spike = self.spiking_net.spike_encode(membrane_potential)\n",
        "            spikes.append(spike)\n",
        "            membrane_potential = membrane_potential + self.spiking_net(spike)\n",
        "        return torch.stack(spikes, dim=0).sum(dim=0)\n",
        "\n",
        "    def spike_encode(self, x):\n",
        "        spikes = F.relu(x)  # Apply ReLU activation\n",
        "        spikes = (spikes >= self.spiking_net.spike_threshold).float()\n",
        "        return spikes\n",
        "\n",
        "def train_150(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 예측 오류 계산\n",
        "        pred = model(inp, time_steps=150)\n",
        "        loss = loss_fn(pred, outp)\n",
        "        \n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_150_5.csv', 'a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss]) \n",
        "\n",
        "def train_250(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 예측 오류 계산\n",
        "        pred = model(inp, time_steps=150)\n",
        "        loss = loss_fn(pred, outp)\n",
        "        \n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_150_5.csv', 'a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss])   \n",
        "\n",
        "def train_300(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 예측 오류 계산\n",
        "        pred = model(inp, time_steps=150)\n",
        "        loss = loss_fn(pred, outp)\n",
        "        \n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_150_5.csv', 'a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss])          \n",
        "                \n",
        "def train_500(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 예측 오류 계산\n",
        "        pred = model(inp, time_steps=150)\n",
        "        loss = loss_fn(pred, outp)\n",
        "        \n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_150_5.csv', 'a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss])       \n",
        "                \n",
        "def train(dataloader, model, loss_fn, optimizer, epochs):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (inp, outp) in enumerate(dataloader):\n",
        "        inp, outp = inp.to(device), outp.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 예측 오류 계산\n",
        "        pred = model(inp)  # Remove the time_steps argument\n",
        "        loss = loss_fn(pred, outp)\n",
        "        \n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 10 == 0:\n",
        "            # torch.save(model, f'./230522_Rand_KP2_PI-DNN_{epochs}_{batch}.pt')\n",
        "            loss, current = loss.item(), batch * len(inp)\n",
        "            print(f\"loss: [{loss:>12f}] [{current:>5d}/{size:>5d}]\")\n",
        "            with open('./loss_data_Test_230522_ND_training_250_6.csv', 'a', newline='') as f:\n",
        "                wr = csv.writer(f)\n",
        "                wr.writerow([loss])        \n",
        "print('Data is loaded, Loss defined.')"
      ],
      "metadata": {
        "id": "s-NHwukyA8Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3  # Define the learning rate\n",
        "\n",
        "model = SNN().to(device)\n",
        "loss_func_MSE = nn.MSELoss().to(device)  # Define the loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Define the optimizer\n",
        "\n",
        "for t in range(1):  # Change range to 1 for one epoch\n",
        "    print(f\"Episode 1 {1} Training\")  # Changed to episode 1 for one epoch\n",
        "    start = time.time()\n",
        "    train(dataloader_2, model, loss_func_MSE, optimizer, 1)  # Changed to 1 epoch\n",
        "    print(\"time :\", time.time() - start)\n",
        "    print('\\n')\n",
        "    torch.save(model, f'./230522_KP2_SNN_1_250_7.pt')\n",
        "\n",
        "    with open('./Training Time_230522_ND_250.csv', 'a', newline='') as f:\n",
        "        wr = csv.writer(f)\n",
        "        wr.writerow([time.time() - start])\n",
        "\n",
        "print(\"Done!\")\n"
      ],
      "metadata": {
        "id": "Zy8_7rQ_A_m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5zTmawrgCCI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import csv\n",
        "\n",
        "# ...\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Episode 1 {t+1} Training\")\n",
        "    start = time.time()\n",
        "    train(dataloader_2, model, loss_func_MSE, optimizer, t)\n",
        "    print(\"Time:\", time.time() - start)\n",
        "    print('\\n')\n",
        "    torch.save(model, f'/content/230522_KP2_DNN_{epochs}_250_7.pt')\n",
        "\n",
        "    with open('/content/Training Time_230522_ND_250.csv', 'a', newline='') as f:\n",
        "        wr = csv.writer(f)\n",
        "        wr.writerow([time.time() - start])\n",
        "        print(\"Training Time Saved to CSV:\", time.time() - start)"
      ],
      "metadata": {
        "id": "Hy2754NGBBOo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}